{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## COMP5625M Deep Learning - Practical Assessment [100 Marks]\n",
    "\n",
    "\n",
    "<div class=\"logos\"><img src=\"Comp5625M_logo.jpg\" width=\"220px\" align=\"right\"></div>\n",
    "\n",
    "This assessment is divided into two parts:\n",
    "> 1. Image classification using a convolutional neural network (CNN), training your model, evaluating your model, understanding fine-tuning, handling overfitting  [60 Marks]\n",
    "> 2. Use of a Recurrent Neural Network (RNN) to predict texts for image captioning, and use of large language models (LLMs) for this task [40 Marks]\n",
    "\n",
    "\n",
    "The maximum number of marks for each part is shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 50% of the final grade for the module.\n",
    "\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this coursework, you will:\n",
    "\n",
    "> 1. Understand and implement a convolutional neural network (CNN) and see how this can be used for a classification problem. \n",
    "> 2. Practice building, evaluating, and finetuning your CNN on an image dataset from development to testing stage. \n",
    "> 3. You will learn to tackle overfitting problem using strategies such as data augmentation and drop out.\n",
    "> 4. Compare your model performance and accuracy with others, such as the leaderboard on Kaggle.\n",
    "> 5. Use a Recurrent Neural Network (RNN) to predict the caption of an image from established word vocabularies.\n",
    "> 6. Understand and visualise text predictions for a given image.\n",
    "> 7. Learn to use transformers for image embedding and transformer-based large language models for text embedding learning.\n",
    "\n",
    "\n",
    "### Setup and resources \n",
    "\n",
    "You must work using this provided template notebook.\n",
    "\n",
    "Having a GPU will speed up the training process. See the provided document on Minerva about setting up a working environment for various ways to access a GPU.\n",
    "\n",
    "Please implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n",
    "> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    "> 3. Your selected image from section 2.4.2 \"Failure analysis\"\n",
    "\n",
    "Final note:\n",
    "\n",
    "> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Your student username (for example, ```sc15jb```): "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> double click to respond"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your full name: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> double click to respond"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Image Classification [60 marks]\n",
    "\n",
    "#### Dataset\n",
    "This coursework will use a subset of images from Tiny ImageNet, which is a subset of the [ImageNet dataset](https://www.image-net.org/update-mar-11-2021.php). Our subset of Tiny ImageNet contains **30 different categories**, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from the Kaggle website:\n",
    "\n",
    ">[Direct access of data is possible by clicking here, please use your university email to access this](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/ESF87mN6kelIkjdISkaRow8BublW27jB-P8eWV6Rr4rxtw?e=SPASDB)\n",
    "\n",
    ">[To submit your results on the Kaggle competition. You can also access data here](https://www.kaggle.com/t/d77d518158104932a434a8954ec9ebc2)\n",
    "\n",
    "To access the dataset, you will need an account on the Kaggle website. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n",
    "\n",
    "> 1. Use your **university email** to register a new account.\n",
    "> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb`` (see the ``note`` below)\n",
    "\n",
    "``Note:`` If the name is already taken in Kaggle then please use a similar pseudo name and add a note in your submission with the name you have used in Kaggle. \n",
    "\n",
    "#### Submitting your test result to Kaggle leaderboard \n",
    "The class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard. More information is provided in the related section below.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "\n",
    "[1] [numpy](http://www.numpy.org) is package for scientific computing with python\n",
    "\n",
    "[2] [h5py](http://www.h5py.org) is package to interact with compactly stored dataset\n",
    "\n",
    "[3] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "[4] [pytorch](https://pytorch.org/docs/stable/index.html) is library widely used for bulding deep-learning frameworks\n",
    "\n",
    "Feel free to add to this section as needed. Examples for importing some libraries is provided for you below.\n",
    "\n",
    "You may need to install these packages using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always check your version\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kfR--uYXHdIi"
   },
   "source": [
    "One challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30. \n",
    "\n",
    "### **Overview of image classification:**\n",
    "\n",
    "**1. Function implementation** [10 marks]\n",
    "\n",
    "*   **1.1** PyTorch ```Dataset``` and ```DataLoader``` classes (4 marks)\n",
    "*   **1.2** PyTorch ```Model``` class for 5-layer (up to 7 layers at most) CNN model (6 marks)\n",
    "\n",
    "**2. Model training** [24 marks]\n",
    "*   **2.1** Training on TinyImageNet30 dataset (6 marks)\n",
    "*   **2.2** Generating confusion matrices and ROC curves (6 marks)\n",
    "*   **2.3** Strategies for tackling overfitting (12 marks)\n",
    "    *   **2.3.1** Data augmentation\n",
    "    *   **2.3.2** Dropout\n",
    "    *   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "            \n",
    "**3. Model testing** [10 marks]\n",
    "*   **3.1**   Testing your final model in (2) on test set - code to do this (4 marks)\n",
    "*   **3.2**   Uploading your result to Kaggle  (6 marks)\n",
    "\n",
    "**4. Model Fine-tuning on CIFAR10 dataset** [16 marks]\n",
    "*   **4.1** Fine-tuning your model (initialise your model with pretrained weights from (2)) (6 marks)\n",
    "*   **4.2** Fine-tuning model with frozen base convolution layers (6 marks)\n",
    "*   **4.3** Compare complete model retraining with pretrained weights and with frozen layers. Comment on what you observe. (4 marks) \n",
    "\n",
    "\n",
    "<!-- **5. Model comparison** [16 marks]\n",
    "*   **5.1**   Load pretrained AlexNet and finetune on TinyImageNet30 until model convergence (8 marks)\n",
    "*   **5.2**   Compare the results of your model with pretrained AlexNet on the same validation set. Provide performance values (loss graph, confusion matrix, top-1 accuracy, execution time) (8 marks) -->\n",
    "<!-- \n",
    "**6. Interpretation of results** (14 marks)\n",
    "*   **6.1** Implement grad-CAM for your model and AlexNet (6 marks)\n",
    "*   **6.2** Visualise and compare your results from your model and AlexNet (4 marks)\n",
    "*   **6.3** Provide comment on (4 marks)\n",
    "    - why the network predictions were correct or not correct in your predictions? \n",
    "    - what can you do to improve your results further?\n",
    "\n",
    "**7. Residual connection for deeper network** (9 marks)\n",
    "*   **7.1** Implement a few residual layers in AlexNet and retrain on TinyImageNet30. You can change network size if you wish. (6 marks)\n",
    "*   **7.2** Comment on why such connections are important and why this impacted your results in terms of loss and accuracy (if it did!) (3 marks)\n",
    "\n",
    "**Quality of your report** (2 marks) -->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Implement a CNN network and dataset class [10 marks]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset class (4 marks)\n",
    "\n",
    "Write a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define a CNN model class (6 marks)\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Convolution layers\n",
    "- Activation functions (e.g. ReLU)\n",
    "- Maxpooling layers\n",
    "- Fully connected layers \n",
    "- Loss function\n",
    "- Optimiser\n",
    "\n",
    "*Please note that the network should be at least a few layers for the model to perform well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "# define a CNN Model class - ideally 5-7 convolutional layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model training [24 marks]\n",
    "\n",
    "\n",
    "### 2.1 Train a CNN model - show loss and accuracy graphs side by side (6 marks)\n",
    "\n",
    "Train your model on the TinyImageNet30 dataset. Split the data into train and validation sets to determine when to stop training. Use seed at 0 for reproducibility and test_ratio=0.2 (validation data)\n",
    "\n",
    "Display the graph of training and validation loss over epochs and accuracy over epochs to show how you determined the optimal number of training epochs. A top-*k* accuracy implementation is provided for you below.\n",
    "\n",
    "> Please leave the graph clearly displayed. Please use the same graph to plot graphs for both train and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HelperDL function) -- Define top-*k* accuracy (**new**)\n",
    "def topk_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE --> Running your CNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment on your model and the results you have obtained. This should include the number of parameters for your models and briefly explain why one should use CNN. You could write a few advantages of using CNN over MLP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating confusion matrix and ROC curves (6 marks)\n",
    "- Use your CNN architecture with best accuracy to generate two confusion matrices, one for the training set and another for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way which clearly indicates what percentage of the data is represented in each position.\n",
    "- Display an ROC curve for the two top and two bottom classes with area under the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Strategies for tackling overfitting (12 marks)\n",
    "Using your (final) CNN model perform the strategies below to avoid overfitting problems. You can reuse the network weights from previous training, often referred to as ``fine tuning``. \n",
    "*   **2.3.1** Data augmentation\n",
    "*   **2.3.2** Dropout\n",
    "*   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "\n",
    "> Plot loss and accuracy graphs per epoch side by side for each implemented strategy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Data augmentation (4 marks)\n",
    "\n",
    "> Implement at least five different data augmentation techniques that should include both photometric and geometric augmentations. \n",
    "\n",
    "> Provide graphs and comment on what you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Dropout (4 marks)\n",
    "\n",
    "> Implement dropout in your model.\n",
    "\n",
    "> Provide graphs and comment on your choice of proportion used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Hyperparameter tuning (4 marks)\n",
    "\n",
    "> Use learning rates [0.1, 0.001, 0.0001].\n",
    "\n",
    "> Provide graphs each for loss and accuracy at three different learning rates in a single graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Model testing [10 marks]\n",
    "Online evaluation of your model performance on the test set. \n",
    "\n",
    "> Prepare the dataloader for the testset.\n",
    "\n",
    "> Write evaluation code for writing predictions.\n",
    "\n",
    "> Upload it to Kaggle submission page (6 marks)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1 Test class and predictions (4 marks)\n",
    "\n",
    "> Build a test class, prepare a test dataloader and generate predictions.\n",
    "\n",
    "Create a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. Test data can be downloaded [here](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/data?select=test_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Prepare your submission and upload to Kaggle  (6 marks)\n",
    "\n",
    "Save all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, e.g., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n",
    "\n",
    "The CSV file must contain only two columns: ‘Id’ and ‘Category’ (predicted class ID) as shown below:\n",
    "\n",
    "```txt\n",
    "Id,Category\n",
    "28d0f5e9_373c.JPEG,2\n",
    "bbe4895f_40bf.JPEG,18\n",
    "```\n",
    "\n",
    "The ‘Id’ column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image in test set and 1 row for the headers. [To submit please click here.](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)\n",
    "\n",
    "> You may submit multiple times. We will use your personal top entry for allocating marks for this [6 marks]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 Model Fine-tuning/transfer learning on CIFAR10 dataset  [16 marks]\n",
    "\n",
    "Fine-tuning is a way of applying or utilizing transfer learning. It is a process that takes a model that has already been trained for one given task and then tunes or tweaks the model to make it perform a second similar task. You can perform fine-tuning in the following fashion:\n",
    "\n",
    "- Train an entire model: Start training model from scratch (large dataset, more computation)\n",
    "\n",
    "- Train some layers, freeze others: Lower layer features are general (problem independent) while higher layer features are specific (problem dependent – freeze)\n",
    "\n",
    "- Freeze convolution base and train only last FC layers (small dataset and lower computation) \n",
    "\n",
    "> **Configuring your dataset**\n",
    "   - Download your dataset using ``torchvision.datasets.CIFAR10`` [explained here](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)\n",
    "   - Split training dataset into training and validation set similar to above. *Note that the number of categories here is only 10*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load pretrained AlexNet from PyTorch - use model copies to apply transfer learning in different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Apply transfer learning with pretrained model weights (6 marks)\n",
    "\n",
    "\n",
    "> Configuration 1: No frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your model changes here - also print trainable parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Fine-tuning model with frozen layers (6 marks)\n",
    "\n",
    "> Configuration 2: Frozen base convolution blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your changes here - also print trainable parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Compare above configurations and comment on performances. (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graphs here and please provide comment in markdown in another cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Multimodal data training [40 marks]\n",
    "\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this part of assessment you will:\n",
    "\n",
    "> 1. Use a Recurrent Neural Network (RNN) to predict the caption of an image from established word vocabularies\n",
    "> 2. Understand and visualise text predictions for a given image.\n",
    "> 3. Learn to use transformers for image embedding and transformer-based large language models for text embedding learning. (new)\n",
    "\n",
    "#### Dataset\n",
    "This assessment will use a subset of the [COCO \"Common Objects in Context\" dataset](https://cocodataset.org/) for image caption generation. COCO contains 330K images, of 80 object categories, and at least five textual reference captions per image. Our subset consists of nearly 5070 of these images, each of which has five or more different descriptions of the salient entities and activities, and we will refer to it as COCO_5070.\n",
    "\n",
    "To download the data:\n",
    "\n",
    "> 1. **Images and annotations**: download the zipped file provided in the link here as [``COMP5625M_data_assessment_2.zip``](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/EWWzE-_AIrlOkvOKxH4rjIgBF_eUx8KDJMPKM2eHwCE0dg?e=DdX62H). \n",
    "\n",
    "``Info only:`` To understand more about the COCO dataset you can look at the [download page](https://cocodataset.org/#download). We have already provided you with the \"2017 Train/Val annotations (241MB)\" but our image subset consists of fewer images compared to orginial COCO dataset. So, no need to download anything from here! \n",
    "\n",
    "> 2. **Image meta data**: as our set is a subset of full COCO dataset, we have created a CSV file containing relevant meta data for our particular subset of images. You can download it also from Drive, \"coco_subset_meta.csv\" at the same link as 1.\n",
    "\n",
    "#### Submission\n",
    "\n",
    "You can either submit the same file or make two separate .ipython notebook files zipped in the submission (please name as ``yourstudentusername_partI.ipynb`` and ``yourstudentusername_partII.ipynb``). \n",
    "\n",
    "**Final note:**\n",
    "\n",
    "> **Please include in this notebook everything that you would like to be marked, including figures. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle of our image-to-text model is as pictured in the diagram below, where an Encoder network encodes the input image as a feature vector by providing the outputs of the last fully-connected layer of a pre-trained CNN (we use [ResNet50](https://arxiv.org/abs/1512.03385)). This pretrained network has been trained on the complete ImageNet dataset and is thus able to recognise common objects. \n",
    "\n",
    "You can alternatively use the COCO trained pretrained weights from [PyTorch](https://pytorch.org/vision/stable/models.html). One way to do this is use the \"FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\" but use e.g., \"resnet_model = model.backbone.body\". Alternatively, you can use the checkpoint from your previous coursework where you fine-tuned to COCO dataset. \n",
    "\n",
    "These features are then fed into a Decoder network along with the reference captions. As the image feature dimensions are large and sparse, the Decoder network includes a linear layer which downsizes them, followed by *a batch normalisation layer to speed up training*. Those resulting features, as well as the reference text captions, are then passed into a recurrent network (we will use an **RNNs** in this assessment). \n",
    "\n",
    "\n",
    "### Sample RNN image captioning\n",
    "The reference captions used to compute loss are represented as numerical vectors via an **embedding layer** whose weights are learned during training.\n",
    "\n",
    "<!-- ![Encoder Decoder](comp5625M_figure.jpg) -->\n",
    "\n",
    "<div>\n",
    "<center><img src=\"comp5625M_figure_v3.jpg\" width=\"1000\"/></center>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for creating vocabulary \n",
    "\n",
    "A helper function file ``helperDL.py`` is provided that includes all the functions that will do the following for you. You can easily import these functions in the exercise, most are already done for you!  \n",
    "\n",
    "> 1. Extracting image features (a pre-trained checkpoint is provided ``coco_features_{cpu/gpu}.pt`` for you to download and use it for training your RNN)\n",
    "> 2. Text preparation of training and validation data is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature map provided to you\n",
    "features_map = torch.load('coco_features_'+(device.type)+'.pt', map_location=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Train an RNN-based image captioning [20 marks]\n",
    "\n",
    "Most of the background codes are provided to you. You only will require to write down the code based on your understanding to:\n",
    "\n",
    "> 5.1 Design a RNN-based decoder. (8 marks)\n",
    "\n",
    "> 5.2 Train your model with precomputed features. (6 Marks)\n",
    "\n",
    "> 5.3 Train using new parameters. (6 Marks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# captions.csv is provided to you\n",
    "# coco/images/ folder is provided to you\n",
    "\n",
    "captions_path = './'\n",
    "image_path  = \"coco/images/\"\n",
    "df = pd.read_csv(captions_path+\"captions.csv\")\n",
    "\n",
    "#visualise\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: For RNN you will need to clean the text, for e.g., converting all uppercases to lowercases \n",
    "# gen_clean_captions_df fuction will do this for you.\n",
    "\n",
    "df[\"clean_caption\"] = \"\"\n",
    "from helperDL import gen_clean_captions_df\n",
    "df_clean = gen_clean_captions_df(df)\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Spilt your training, validation and test dataset with indexes to each set\n",
    "from helperDL import split_ids\n",
    "train_id, valid_id, test_id = split_ids(df['id'].unique())\n",
    "print(\"training:{}, validation:{}, test:{}\".format(len(train_id), len(valid_id), len(test_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = df.loc[df['id'].isin(train_id)]\n",
    "valid_set = df.loc[df['id'].isin(valid_id)]\n",
    "test_set = df.loc[df['id'].isin(test_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### word 2 an integer ID - build vocabulary\n",
    "class Vocabulary(object):\n",
    "    \"\"\" Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    def __init__(self):\n",
    "        # intially, set both the IDs and words to dictionaries with special tokens\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1, '<end>': 2}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<end>'}\n",
    "        self.idx = 3\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # if the word does not already exist in the dictionary, add it\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            # increment the ID for the next word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # if we try to access a word not in the dictionary, return the id for <unk>\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build vocabulariy for each set - train, val and test \n",
    "# you will be using to create dataloaders\n",
    "from helperDL import build_vocab\n",
    "\n",
    "# create a vocab instance\n",
    "vocab = Vocabulary()\n",
    "vocab_train = build_vocab(train_id, df_clean, vocab)\n",
    "vocab_valid = build_vocab(valid_id, df_clean, vocab)\n",
    "vocab_test = build_vocab(test_id, df_clean, vocab)\n",
    "\n",
    "vocab = vocab_train # using only training samples as vocabulary as instructed above\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They can also join the train and valid captions but they will need to run vocabulary after concatenation\n",
    "import numpy as np\n",
    "vocab = build_vocab(np.concatenate((train_id, valid_id), axis=0), df_clean, vocab) #---> overrighting\n",
    "len(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a ```DataLoader``` for your image feature and caption dataset. ``helperDL.py`` file includes all the required functions\n",
    "\n",
    "We need to overwrite the default PyTorch collate_fn() because our \n",
    "ground truth captions are sequential data of varying lengths. The default\n",
    "collate_fn() does not support merging the captions with padding.\n",
    "\n",
    "You can read more about it here:\n",
    "https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperDL import EncoderCNN  \n",
    "model = EncoderCNN() \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load the saved image feature maps and trained model (\"provided to you\") '''\n",
    "\n",
    "# ---> your entry here (make sure that the path is correct - load the coco_features_{cpu/gpu}.pt)\n",
    "features = torch.load(\"coco_features.pt\")\n",
    "\n",
    "# ---> load the model ckpt and udate the model state dict of the base model\n",
    "# your entry here (make sure that the path is correct)\n",
    "checkpoint = torch.load(\"ckpt file here\") \n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Preparing the train, val and test dataloaders\n",
    "from helperDL import COCO_Features\n",
    "from helperDL import caption_collate_fn\n",
    "\n",
    "# Create a dataloader for train\n",
    "dataset_train = COCO_Features(\n",
    "    df=train_set,\n",
    "    vocab=vocab,\n",
    "    features=features_map,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0, # may need to set to 0\n",
    "    collate_fn=caption_collate_fn, # explicitly overwrite the collate_fn\n",
    ")\n",
    "\n",
    "# Create a dataloader for valid\n",
    "dataset_valid = COCO_Features(\n",
    "    df=valid_set,\n",
    "    vocab=vocab,\n",
    "    features=features_map,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0, # may need to set to 0\n",
    "    collate_fn=caption_collate_fn, # explicitly overwrite the collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration file - You will be asked below to train your RNN using various configurations and compare.\n",
    "# --> Please change these numbers as required. \n",
    "# --> Please comment on changes that you do.\n",
    "\n",
    "class cfg:\n",
    "    EMBED_SIZE = 256\n",
    "    HIDDEN_SIZE = 512\n",
    "    NUM_LAYERS = 1\n",
    "    LR = 0.001\n",
    "    NUM_EPOCHS = 5\n",
    "    LOG_STEP = 10\n",
    "    MAX_SEQ_LEN = 37"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Design a RNN-based decoder (8 marks)\n",
    "\n",
    "Read through the ```DecoderRNN``` model below. First, complete the decoder by adding an ```RNN``` layer to the decoder where indicated, using [the PyTorch API as reference](https://pytorch.org/docs/stable/nn.html#rnn).\n",
    "\n",
    "Keep all the default parameters except for ```batch_first```, which you may set to True.\n",
    "\n",
    "In particular, understand the meaning of ```pack_padded_sequence()``` as used in ```forward()```. Refer to the [PyTorch ```pack_padded_sequence()``` documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=cfg.EMBED_SIZE, hidden_size=cfg.HIDDEN_SIZE, num_layers=cfg.NUM_LAYERS, max_seq_length=cfg.MAX_SEQ_LEN):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # we want a specific output size, which is the size of our embedding, so\n",
    "        # we feed our extracted features from the last fc layer (dimensions 1 x 2048)\n",
    "        # into a Linear layer to resize\n",
    "        self.resize = nn.Linear(2048, embed_size)\n",
    "        \n",
    "        # batch normalisation helps to speed up training\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # ---> RNN (use nn.RNN function passing arguments that you computed above, use batch_first=True as an argument)\n",
    "        self.rnn =                                       #... your code here\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "\n",
    "        # a) compute the embedding of caption (Complete the code!)\n",
    "        embeddings =\n",
    "        # b) resize the image features (Complete the code!)\n",
    "        im_features = \n",
    "        # c) apply batch norm on image the resized feature (Complete the code!)\n",
    "        im_features =\n",
    "        # d) concatenate the image feature and the embeddings - use unsqueeze(1) to make the appropriate  size\n",
    "        embeddings = torch.cat((im_features.unsqueeze(1), embeddings), 1)\n",
    "        # e) use pack_padded_sequence \n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.rnn(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "\n",
    "        inputs = self.bn(self.resize(features)).unsqueeze(1)\n",
    "        for i in range(self.max_seq_length):\n",
    "            hiddens, states = self.rnn(inputs, states)  # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))   # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)               # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)              # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)       # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate decoder\n",
    "decoder = DecoderRNN(len(vocab), embed_size=cfg.EMBED_SIZE, hidden_size=cfg.HIDDEN_SIZE, num_layers=cfg.NUM_LAYERS).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Train your model with precomputed features [6 marks]\n",
    "\n",
    "Train the decoder by passing the features, reference captions, and targets to the decoder, then computing loss based on the outputs and the targets. Note that when passing the targets and model outputs to the loss function, the targets will also need to be formatted using ```pack_padded_sequence()```.\n",
    "\n",
    "We recommend a batch size of around 64 (though feel free to adjust as necessary for your hardware).\n",
    "\n",
    "**We strongly recommend saving a checkpoint of your trained model after training so you don't need to re-train multiple times.**\n",
    "\n",
    "Display a graph of training and validation loss over epochs to justify your stopping point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define: loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=cfg.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # train the models\n",
    "    total_step = len(train_loader)\n",
    "    total_step_v = len(valid_loader)\n",
    "    stats = np.zeros((cfg.NUM_EPOCHS,2))\n",
    "    print(stats.shape)\n",
    "    total_loss = 0\n",
    "    for epoch in range(cfg.NUM_EPOCHS):\n",
    "        for i, (features_, captions_, lengths_) in enumerate(train_loader):\n",
    "\n",
    "            # set mini-batch dataset\n",
    "            features_ = features_.to(device)\n",
    "            captions_ = captions_.to(device)\n",
    "            targets = pack_padded_sequence(captions_, lengths_, batch_first=True)[0]\n",
    "            decoder.zero_grad()\n",
    "\n",
    "            # forward, backward and optimize\n",
    "            outputs = decoder(features_, captions_, lengths_)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # print stats\n",
    "            if i % cfg.LOG_STEP == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{cfg.NUM_EPOCHS}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        stats[epoch,0] = round(total_loss/total_step,3)\n",
    "        total_loss = 0\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():  \n",
    "            for i, (features_, captions_, lengths_) in enumerate(valid_loader):\n",
    "\n",
    "                # set mini-batch dataset\n",
    "                features_ = features_.to(device)\n",
    "                captions_ = captions_.to(device)\n",
    "                targets = pack_padded_sequence(captions_, lengths_, batch_first=True)[0]\n",
    "\n",
    "                # forward, backward and optimize\n",
    "                outputs = decoder(features_, captions_, lengths_)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        stats[epoch,1] = round(total_loss/total_step_v,3)\n",
    "        total_loss = 0\n",
    "        # print stats\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Epoch [{epoch+1}/{cfg.NUM_EPOCHS}], Train_Loss: {stats[epoch,0]}, Valid_Loss: {stats[epoch,1]}\")\n",
    "        print(\"=\"*30)\n",
    "        decoder.train()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(stats, figure_name=\"coco_train_vocab_only.png\"):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(stats[:,0], 'r', label = 'training', )\n",
    "    plt.plot(stats[:,1], 'g', label = 'validation' )\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(f\"COCO dataset- train vocab only #vocab={len(vocab)}\")\n",
    "    fig.savefig(figure_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train using default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = train()\n",
    "plot_stats(stats, \"coco_default_parameters.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Train using new parameters (6 Marks)\n",
    "\n",
    "> In CFG class change --> NUM_LAYERS = 2 and  EMBED_SIZE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model after training\n",
    "#your code here! e.g.: decoder_ckpt = torch.save(decoder, \"coco_subset_assessment_decoder.ckpt\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Test prediction and evaluation [8 marks] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Generate predictions on test data (4 marks)\n",
    "\n",
    "Display 4 sample test images containing different objects, along with your model’s generated captions and all the reference captions for each.\n",
    "\n",
    "> Remember that everything **displayed** in the submitted notebook and .html file will be marked, so be sure to run all relevant cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperDL import COCOImagesDataset\n",
    "\n",
    "data_transform = transforms.Compose([ \n",
    "    transforms.Resize(224),     \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "dataset_test = COCOImagesDataset(\n",
    "    df=test_set,\n",
    "    transform=data_transform,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "decoder.eval() # generate caption, eval mode to not influence batchnormncoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting functions from helperDL.py\n",
    "from helperDL import timshow\n",
    "from helperDL import decode_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_TO_SHOW = 4\n",
    "idx = 0\n",
    "with torch.no_grad():\n",
    "    for i, (image,filename) in enumerate(test_loader):\n",
    "        \n",
    "        # your code here --->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        print(f\"GENERATED: \\n\")\n",
    "        print(\"REFERENCES:\")\n",
    "\n",
    "        print(\"===================================\\n\")\n",
    "\n",
    "\n",
    "        timshow(image[0].cpu())\n",
    "        idx +=1\n",
    "        if idx == IMAGES_TO_SHOW:\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Caption evaluation using cosine similarity (4 marks)\n",
    "\n",
    "The cosine similarity measures the cosine of the angle between two vectors in n-dimensional space. The smaller the angle, the greater the similarity.\n",
    "\n",
    "To use the cosine similarity to measure the similarity between the generated caption and the reference captions: \n",
    "\n",
    "* Find the embedding vector of each word in the caption \n",
    "* Compute the average vector for each caption \n",
    "* Compute the cosine similarity score between the average vector of the generated caption and average vector of each reference caption\n",
    "* Compute the average of these scores \n",
    "\n",
    "Calculate the cosine similarity using the model's predictions over the whole test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Use language models for image captioning [12 marks]\n",
    "\n",
    "Description: A large language model (LLM) is a type of artificial intelligence (AI) model that uses deep learning to perform natural language processing (NLP) tasks.\n",
    "\n",
    "One way to train on multimodal (image and text data) is CLIP (Contrastive Language-Image Pre-Training). CLIP is a neural network trained on a variety of (image, text) pairs and can take CNNs or transformers for encoding images and widely accepted LLMs for text embedding. Paper here: https://arxiv.org/pdf/2103.00020\n",
    "\n",
    "CLIP is trained on flickr 30K dataset. First we will try on flickr 8k data then on COCO.\n",
    "\n",
    "> Most codes are provided for you. \n",
    "> Small model and large pre-trained models are given to you and also we have provided the configS and configL that are required to run these models.\n",
    "\n",
    "> **Your task is to:**\n",
    "\n",
    "    - a. Understand how the smaller model perform on the flickr 8k and COCO dataset (2 images each) [4 marks]\n",
    "\n",
    "    - b. Understand how the larger model perform on the flickr 8k and COCO dataset (2 images each) [4 marks]\n",
    "    \n",
    "    - c. Finally, understand how the CLIP model trained on a larger curated COCO dataset generalises to the COCO dataset (use same 2 images from above) [4 marks]\n",
    "\n",
    "You can find the link to download small and large pretrained CLIP models also here: [CLIP_S_L_Models](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/ERX5QK77b81Jv4K_5ff58ekBsXIs6EAL7tC4Ge1COMMRrA?e=gnzmX7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#configuration for the smaller model (slightly over 1 GB model weight file)\n",
    "\n",
    "class configS:\n",
    "    \"\"\"\n",
    "    Project's main config.\n",
    "    \"\"\"\n",
    "\n",
    "    clip_model: str = \"openai/clip-vit-base-patch32\"\n",
    "    text_model: str = \"gpt2\"\n",
    "    seed: int = 100\n",
    "    num_workers: int = 2\n",
    "    train_size: int = 0.84\n",
    "    val_size: int = 0.13\n",
    "    epochs: int = 150\n",
    "    lr: int = 3e-3\n",
    "    k: float = 0.33\n",
    "    batch_size_exp: int = 6\n",
    "    ep_len: int = 4\n",
    "    num_layers: int = 6\n",
    "    n_heads: int = 16\n",
    "    forward_expansion: int = 4\n",
    "    max_len: int = 40\n",
    "    dropout: float = 0.1\n",
    "    weights_dir: str = os.path.join(\"weights\", \"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration for the larger model (nearly 3 GB model weight file)\n",
    "class configL:\n",
    "    \"\"\"\n",
    "    Project's main config - large.\n",
    "    \"\"\"\n",
    "\n",
    "    clip_model: str = \"openai/clip-vit-large-patch14\"\n",
    "    text_model: str = \"gpt2-medium\"\n",
    "    seed: int = 100\n",
    "    num_workers: int = 2\n",
    "    train_size: int = 0.84\n",
    "    val_size: int = 0.13\n",
    "    epochs: int = 120\n",
    "    lr: int = 5e-3\n",
    "    k: float = 0.3\n",
    "    batch_size_exp: int = 5\n",
    "    ep_len: int = 4\n",
    "    num_layers: int = 5\n",
    "    n_heads: int = 16\n",
    "    forward_expansion: int = 4\n",
    "    max_len: int = 40\n",
    "    dropout: float = 0.08\n",
    "    weights_dir: str = os.path.join(\"weights\", \"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_CLIP import Net # function is given to you\n",
    "\n",
    "model = Net(\n",
    "    clip_model=configS.clip_model,\n",
    "    text_model=configS.text_model,\n",
    "    ep_len=configS.ep_len,\n",
    "    num_layers=configS.num_layers,\n",
    "    n_heads=configS.n_heads,\n",
    "    forward_expansion=configS.forward_expansion,\n",
    "    dropout=configS.dropout,\n",
    "    max_len=configS.max_len,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "#use small model\n",
    "checkpoint = torch.load('model_S.pt', map_location=device) \n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "# model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Try on COCO dataset with the small model only - Comment on your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Output on two images of your choice from 'flickr 8k' \n",
    "# img = Image.open('Images/55473406_1d2271c1f2.jpg')\n",
    "# img = Image.open('Images/260828892_7925d27865.jpg')\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# Output on two images of your choice from 'COCO' dataset\n",
    "# img = Image.open(image_path + './000000000030.jpg')\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "\n",
    "temperature = 1.0\n",
    "with torch.no_grad():\n",
    "    caption, _ = model(img, temperature)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(caption)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<--- Your comment here --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Repeat the above with the larger model \"model_L\" - use large model configuration. Comment on your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<--- Your comment here --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Try on COCO dataset with the large model only - Comment on your observation. What would you have to do to improve network performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<- Your comment here ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now try on the CLIP fine-tuned on High-Level dataset. \n",
    "\n",
    "The High-Level (HL) dataset aligns object-centric descriptions from COCO with high-level descriptions crowdsourced along 3 axes: scene, action, rationale.\n",
    "\n",
    "The HL dataset contains 14997 images from COCO and a total of 134973 crowdsourced captions (3 captions for each axis) aligned with ~749984 object-centric captions from COCO.\n",
    "\n",
    "https://huggingface.co/datasets/michelecafagna26/hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run these to install CLIPClap - fine tuned on COCO\n",
    "\n",
    "!pip install git+https://github.com/michelecafagna26/CLIPCap.git\n",
    "\n",
    "!git lfs install \n",
    "!git clone https://huggingface.co/michelecafagna26/clipcap-base-captioning-ft-hl-scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipcap import ClipCaptionModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import clip\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "model_path = \"clipcap-base-captioning-ft-hl-scenes/pytorch_model.pt\" # change accordingly\n",
    "\n",
    "# load clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "prefix_length = 10\n",
    "\n",
    "# load ClipCap\n",
    "model = ClipCaptionModel(prefix_length, tokenizer=tokenizer)\n",
    "model.from_pretrained(model_path)\n",
    "model = model.eval()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the image\n",
    "# raw_image = Image.open('Images/260828892_7925d27865.jpg').convert('RGB')\n",
    "# raw_image = Image.open(image_path + './000000000030.jpg').convert('RGB')\n",
    "\n",
    "# Your code here --->\n",
    "\n",
    "# extract the prefix\n",
    "img = preprocess(raw_image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    prefix = clip_model.encode_image(img).to(\n",
    "        device, dtype=torch.float32\n",
    "    )\n",
    "    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "\n",
    "# generate the caption   \n",
    "caption = model.generate_beam(embed=prefix_embed)[0]\n",
    "\n",
    "plt.imshow(raw_image)\n",
    "plt.title(caption)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<--- Your comment here --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Your answer here>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thank you for completing the practical assessment - if you have any question, please ask on teams channel or attend lab sessions on Wednesdays and Thursdays."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
